\subsection{Experimental Testbed}
In this section, we employ RambutanAcc to study the impact of various code optimizations at different hardware levels of a GPU cluster.
Since an application generally benefits from only a subset of these optimizations, we need more than one application.
To this end, we take four benchmarks commonly used in science and engineering: sparse cholesky factorization, a machine learning based face recognition code, 2.5D Cannon's Matrix Multiplication, and a 3D Stencil.
When studying optimizations on a single GPU, to have apples-to-apples comparisons we always use our own CUDA kernels instead of 3rd party implementations (e.g. cuSPARSE). 
In particular, for Sparse Cholesky Factorization, we develop our own CUDA kernels for $C = \alpha* A * B + \beta C$ where either A, B, and C are all in the {\em sparse} form or A is dense and B and C are sparse.
In these kernels, we perform many optimization techniques such as memory coalescing and spatial tiling.
We also employ a warp based thread reduction implementation that minimizes memory accesses. 
For studies at higher hardware levels, we use vendor provided kernels as much as possible.
For example, in  2.5D Cannon we use the cuBLAS's {\em dgemm} and {\em daxpy} implementations included in the NVIDIA toolkit to perform local matrix multiplication.

For node level optimizations (on single and multiple GPUs of the same compute node), we use up to three K80 GPUs.
A K80 GPU pairs two GPU devices each having 2496 CUDA cores organized into 13 SMs.
The whole K80-based system is equipped with 24GB DRAM with up to 480 GB/s memory bandwidth.
For cluster level optimizations, we run code on K20 GPUs on Titan.
We use the nvcc compiler version 7.0 and sm\_35 capability for CUDA codes and the Intel compiler for codes running on the host.
We use GASNet for communication among GPUs. 


\subsection{Scheduling tasks on SMs of a GPU}

\subsubsection{Sparse Cholesky Factorization}
Sparse Cholesky Factorization A= $LL^T$, where A is a sparse and symmetric positive-definite matrix,
appears in many scientific and engineering problems.
Depending on the sparsity pattern of the input matrix, many sparse representations can be used.
In this paper we employ the CSC (Compressed Sparse Column) format. 
The input matrix is organized as a list of "non-zero" tiles, each including lists of non-zero elements and their row and column indices.
The factorization operation is comprised of three smaller kernels: {\em factor}, {\em solve}, and {\em update}.
These computations on CSC tiles and their data dependencies can be represented by a DAG as shown in Fig.~\ref{fig:cholesky}. 
For very sparse matrices, this DAG may consist of many small tasks.
Thus, this is a perfect application to evaluate the benefit of the fine-grained task scheduling support of the runtime.
We place data on the host's DRAM and execute {\em factor} and {\em solve} on the host's worker.
The compute-intensive {\em update} kernel is executed on the GPU workers.
{\em RambutanAcc} automatically streams data required by this kernel to the GPU's DRAM and streams the results back.

\begin{figure}[htb]
\centering
\includegraphics[width=.3\textwidth]{figures/cholesky.pdf}
\caption{Cholesky factorization DAG consisting of three types of tasks: F (Factor), S (Solve), U (update). Each task is associated with a partition of the input matrix called a {\em tile}. Arrows presesent data dependencies between tasks of different types or of the same task type but on different tiles.}
\label{fig:cholesky}
\end{figure}


An interesting path to explore is the tradeoff between coarse and fine-grained scheduling policies.  
For sparse cholesky, the matrix is represented by many small CSC tiles.\samW{the previous sentence repeats the one}
Thus, even a small problem size can result in many tasks.
Fig.~\ref{fig:coarseFine} shows results of sparse cholesky under two scheduling policies.
It can be seen that fine-grained scheduling policy outperforms the coarse-grained one.
This can be explained as follows.
Each CSC tile is very small (e.g. 32$\times$32), making it hard to map computations efficiently to many CUDA cores.
Thus, it may not be posible to scale a task to all available SMs of a GPUs.
We observe that for many input matrices tasks run more efficiently after reducing the number of SMs per worker by a factor of 2$\times$ or 4$\times$.
Since there are many tasks that can be runnable at a time, the scheduler can keep all SMs busy at very small overhead.
Fig~\ref{fig:nWorkers} shows the optimal number of workers on a K80 GPU for Sparse Cholesky Factorization when the degree of sparsity of the input matrix varies.
If the sparse matrix is filled with many tasks we have more parallelism, allowing us to configure the GPUs with more workers.

The lesson learn from this study is that fine-grained scheduling can be very helpful if we have a DAG with many small tasks, which can not run well on the whole GPU-based system.
This is an important observation since sparse representation is very common in practice.
{\em RambutanAcc} runtime supports fine-grained task scheduling, a simple yet powerful solution to this problem.
The programmer can obtain high compute throughput on GPUs without complicating the application algorithm.

\begin{figure}[htb]
\centering
\begin{subfigure}{0.23\textwidth}
\includegraphics[width=\textwidth]{figures/choleskyScheResults.pdf}
\caption{Tile size 32$\times$32}
\label{choleskySche}
\end{subfigure}
\begin{subfigure}{0.23\textwidth}
\includegraphics[width=\textwidth]{figures/nWorkers.pdf}
\caption{The optimal number of workers/GPUs}
\label{fig:nWorkers}
\end{subfigure}
\caption{Coarse vs. fine-grained Scheduling\samW{note left vs. right;  for left, what is the sparsity???  where does the matrix come from???}}
\label{fig:coarseFine}
\end{figure}


\subsubsection{Viola-Jones Face Recognition}
\samW{is there a citation for this code/algorithm}
We next study the impact of dynamic task scheduling in balancing the workload among SMs of a GPU.
For this study we use the Viola-Jones face detection kernel, an important module in many applications such as security surveillance.
The Viola-Jones face detection algorithm detects faces by scanning a regtangular window of pixels over the image where it looks for features of a human face. 
If a window contains a significant number of these features, it is considered to be a face. 
Since face size varies, the window is scaled a number of times and the scanning process is repeated. 
To reduce the number of features that each window needs to check, the window passes through a number of different stages. 
Early stages have fewer features to check and are easier to pass whereas later stages have more features and are more selective. 
At each stage, the calculations of features are accumulated and, if this accumulated value does not pass the threshold, the stage is failed and the current window is considered to not contain a face. 

For this application, it is straightforward to exploit parallelism among search windows.
Each CUDA thread block is responsible for a fixed number of windows, which will be further distributed to threads within the thread block.
We call this code variant {\em CUDA-Basic}.
Since the number of instructions per window depends on the input, the impact of thread divergion is expected to be significant.
Thus, we also employ a {\em CUDA-Static Warp Scheduling} version which allows 32 threads in a warp to share a window (and thus they perform the same number of instructions).
Porting this code on RambutanAcc, we expect to improve the performance further by balancing the workload among these warps.
Finally, we run a hand-optimized code variant, which embeds the task scheduler into the kernel code.
This code has all the capabilities that RambutanAcc can, but at lower cost since the task scheduler is specialized and runs on the GPU.

\begin{figure}[htb]
\centering
\includegraphics[width=0.35\textwidth]{figures/faceRecognition.pdf}
\caption{Balance the face searching\samW{what is FPS?  I assume up is good... i.e. FPS is a throughput rather than time metric}}
\label{faceRecognition}
\end{figure}

Fig.~\ref{faceRecognition} shows the performance of all code variants.
{\em CUDA-Basic} performs poorly as expected.
Under this straightforward strategy, only one or a few threads among 32 threads of the same warp reach to late stages while the rest stay idle.\samW{previous sentence is unclrear...  Are you saying the naive code has a thread divergence problem?}
This explains why {\em CUDA-Static Warp Scheduling} improves the performance significantly.
However, there remains significant load imbalance among warps.
With RambutanAcc, windows are distributed to workers dynamically.
Specifically, we configure the runtime with 26 workers (two workers per SM), each a CUDA thread block. 
The scheduler running on the host keeps assigning blocks of windows to these workers.
In order to hide the scheduling latency, we configure the task buffer of each worker with multiple slots.
While the worker is processing the current window block, the scheduler can offload another block to the remaining slots.
In this experiment, each block takes about 50$\mu$s to finish, whereas the offloading cost is around 10$\mu$s.
Thus, configuring the task buffer with two slots is sufficient.
Fig.~\ref{faceRecognition} shows that RambutanAcc speeds up {\em CUDA-Static Warp Scheduling} by 1.35$\times$.
All the performance improvement can be attributed to the capability of balancing computationgs among SMs of the GPU.
The {\em CUDA-Hand Optimized} version runs even faster.
The additional performance improvement is due to reducing scheduling overhead by embedding the scheduler to the application source code.
\samW{where did the CUDA versions of the code come from?  Did you write them all?  if not, cite source}


\subsection{Scheduling tasks on GPUs of the same node}

\subsubsection{3D Stencil}
\samW{3D stencil is an ill-defined term... is this a 7-point, constant coefficient laplacian?  Are you doing a smoother with a RHS?  are you assuming periodic BCs?  or constant(node/vertex centerd) boundaries?}
On multiple GPUs we pick {\em 3D Stencil}, an iterative solver for Laplace's equation in three dimensions.
{\em 3D Stencil} iterates over a 3D mesh, updating data elements using values from six nearest neighbors.
The DAG for this application is similar to that shown earlier in Fig.~\ref{fig:taskGraph}, except for the number of dimensions.
In particular, each task is associated with a data partition with up to six ghost cells.
A task can be run when the previous iteration on this data partition finishes and it pulls all the needed ghost cells from neighboring tasks.
3D stencil is a memory bandwidth bound application. 
Thus using GPUs can boost up the performance significantly.
It is interesting to determine if {\em RambutanAcc} can improve the performance further by  hiding communication overheads.

\begin{figure*}[htb]
\begin{subfigure}[b]{0.47\textwidth}
\centering
\includegraphics[width=0.9\textwidth]{figures/stencil_single_node_tida.pdf}
\caption{A compute node with 6 Kepler GPUs (3 K80 cards) connected via a shared PCIe bus}
\label{stencil_single_node_tida}
\end{subfigure}
\begin{subfigure}[b]{0.47\textwidth}
\centering
\includegraphics[width=0.9\textwidth]{figures/stencil_single_node_summit.pdf}
\label{stencil_single_node_summit}
\caption{A compute node with 4 Pascal (P100) GPUs. GPUs \#1 and \#2, \#3 and \#4 are paired togethor via an NVlink interconnect.}
\end{subfigure}
\caption{Strong scaling study on single compute nodes. Problem size $512^3$.
%\samW{I suggest the x-axis be GPU devices (2,4,6).  It also makes sense for there to be a single device (no communication) data point.}
}
\end{figure*}




\subsubsection{2.5D Cannon Matrix Multiply}
Although sparse representations are widely used in practice, dense matrix operations also have a significant share in many scientific and engineering areas.
As a result, we employ a dense matrix multiplication operation $C = \alpha* A * B + \beta C$ to evaluate our runtime.
This is a compute bound application, and the GPU architecture is very well suited for the computation. 
There are many algorithms for the matrix multiply operation, and we use a well-known extension of the standard 2D Cannon's algorithm called {\em Communication Avoiding} or 2.5D Cannon~\cite{25Dcannon}. 
Under the original 2D Cannon's algorithm, the available tasks are organized into a {\em T=PxP} mesh, partitioning each of the three matrices A, B, and C into blocks.
These partitions are first aligned using a skewing operation.
The algorithm then performs P computation steps accumulating the C partition using the rotated A and B partitions.
The communication avoiding algorithm shown in Fig.~\ref{fig:25DCannon} replicates the input matrices by a factor of L using an additional task dimension.
The algorithm broadcasts input data to layers in this dimension to compute the traditional Cannon with T/$\sqrt(L^3)$ steps then reduces the results back to the first layer.

\begin{figure*}[htb]
\centering
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{figures/cannon0.pdf}
\caption{A and B dependencies on each replication layer}
\label{deps}
\end{subfigure}
\begin{subfigure}[b]{0.37\textwidth}
\includegraphics[width=\textwidth]{figures/cannon1.pdf}
\caption{Results are reduced to the original layer}
\label{dataspace}
\end{subfigure}
\caption{Computing $C= \alpha A * B + \beta C$ using the 2.5D Cannon's matrix multiplication algorithm given the input matrices are already replicated and aligned. 
The DAG partitions matrix C and the step space of the algorithm.
Task Id is a triple where the first two numbers represent the coordinates of a C partition and the last is the step number of the algorithm. 
Fig.~\ref{deps} shows two subsets of the graph to illustrate two types of data dependecies required to compute C.}
\label{fig:25DCannon}
\end{figure*}


\begin{figure}[htb]
\centering
\includegraphics[width=0.49\textwidth]{figures/cannon_tida.pdf}
\caption{2.5 Cannon on two K80s (four GPU devices)}
\label{cannon_onnode}
\end{figure}




\subsection{Scheduling tasks at the cluster level}
%\samW{is this a missing subsection? or is communication hiding supposed to be a subsubsection???}


\begin{figure}[htb]
\centering
\includegraphics[width=0.49\textwidth]{figures/stencil_multiple_nodes_summit.pdf}
\caption{Stencil on multiple nodes of Summit}
\label{stencil_multiple_nodes}
\end{figure}



%\subsubsection{Communication hiding}
%We now extend the experiment to multiple GPUs.
%In this experiment, we evaluate the benefit of hiding the communication overheads among GPUs.
%To this end, we configure the runtime in two modes: {\em no overlap} and {\em overlap}.
%The former uses blocking CUDA memory copy routines to transfer data between host and GPUs while the latter uses non-blocking variants.
%Since the fine-grained scheduler is not compatible with blocking mode (the persistent kernel runs to completion while blocking routines can't proceed until all previously submitted CUDA kernels have completed), we use the coarse-grained scheduling policy for both the blocking and non-blocking modes.

%Fig.~\ref{overlap} shows results of three applications under two communication modes.
%In this study, we do not replicate the input matrices of the 2.5D Cannon's algorithm because the communication avoiding technique may interfere with the communication overlap.
%We will study this interference later in Sec~\ref{subsec:CAvsOlap}.
%It can be seen in Fig.~\ref{overlap} that on three applications {\em overlap} always outperforms {\em no overlap}.
%In Cholesky, we place data on the host and stream them to GPUs to perform the compute-intensive {\em update} kernel.
%Thus, even on one GPU, communication arises.
%\footnote{Although we do not show results of 2.5D Cannon and 3D Stencil on one GPU, it's worth noting that computing on one GPU doesn't incur communication cost since we initially place data on the GPU.
%As a result, we do not observe performance improvement when running these two applications on only one GPU.}
%On multiple GPUs, we realize notable performance improvement via overlapping communcation with computation.
%The overall time reduction is 10\% more or less.
%For Stencil, however, we see a higher speedup (up to 1.85$\times$) due to the following reason.
%At a small scale 1D decomposition works best since it does not require the costly packing and unpacking operations.
%However, with a 1D decomposition scheme the amount of communication does not decrease as the number of GPUs increases.
%Thus, the more GPUs the higher communication relative to computation, resulting in a better improvement due to overlap.
%Unlike 3D Stencil, experiments on the other two applications use a 2D decomposition scheme.
%Thus, the communication over computation ratio does not change much as the number of GPU increases.

%\begin{figure*}[htb]
%\centering
%\includegraphics[width=0.9\textwidth]{figures/overlap.pdf}
%\caption{Hiding communication automatically via overlap
%\samW{How much potential was there for overlap in the first place?}
%%\samW{When strong scaling, there is a region where it might be viable.}
%\samW{When weak scaling, you may always be outside the range where overlap is viable.}
%}
%\label{overlap}
%\end{figure*}

%\begin{figure}[htb]
%\centering
%\includegraphics[width=0.49\textwidth]{figures/CA_4096.pdf}
%\caption{2.5D Cannon on 16 GPUs using small matrices (N=4096). The communication avoiding technique results in many task configurations. For good configurations (e.g. \{64, 4\}), there is not much room for the communication overlap. However, for poor configurations (e.g. \{1024, 4\}) the overlap technique does a good job in further increasing the performance}
%\label{CA_4096}
%\end{figure}

%\subsubsection{Interference between communication avoiding and hiding}
%\label{subsec:CAvsOlap}
%Now let's study the behavior of {\em overlap} when the communication avoiding technique is enabled.
%Fig.~\ref{CA_4096} shows the run time of the 2.5D Cannon program when performing matrix multiplication on 16 GPUs.
%We can see that replicating matrices substantially reduces the run time.
%Notable examples are  \{64, 4\} compared to \{64, 1\} and \{256, 4\} compared to \{256, 1\}.
%If most of the data communication can be avoided, there is not much left to hide.
%However, the number of these optimal replication configurations is very small compared to the combination of task and replication spaces.
%As a result, the programmer may need to brute force many potential configurations to find the best one.
%This requirement is costly and time consuming.
%Luckily the overlapping technique can work with communication avoiding within the same application.
%Thus, if the programmer does not pick the best replication configuration, he/she can rely on the communication overlap to realize comparable performance.
%For example, the {\em overlap} performance on configurations \{128, 2\}, \{256, 4\}, or the most wanted \{64, 1\} is  acceptable. 
%We can see that there are many of such configurations, allowing the programmer to guess one easily.

