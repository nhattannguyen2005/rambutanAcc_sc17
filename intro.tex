It is very likely that exascale systems will be armed with powerful accelerators and/or  many-core processors~\cite{ASCR/Exascale/Lethin,  exascaleRoadMap}.
On such systems, performance will heavily depend on the efficient utilization of accelerators/many-core processors, which often requires aggressive, low-level kernel optimizations that destroy portability across systems with different processor architectures.
To handle the performance portability challenge, the programmer must optimize several versions of the same compute kernel.
This problem is challenging especially for domain programmers, given that they must also optimize the code to run on hundreds or thousands of compute nodes.
In this scenario, many runtime systems have been developed to aid the programmer~\cite{legion,physics,mpiacc,mvapich2gpu}.
However, current support for state-of-the-art accelerators is very limited, and there remain significant challenges, such as the development of programming abstractions and fine-grained scheduling mechanisms, as well as efficient communication overlap and dynamic load balancing.  

To address these challenges, we present {\em RambutanAcc}, a task-based runtime for distributed-memory systems that can assist the programmer to increase performance on accelerator-based clusters with modest programming effort.
{\em RambutanAcc} extends the programming interface of {\em Rambutan} \cite{rambutanWebsite}, which provides an interface to construct a fine-grain, dynamic dataflow graph that unfolds as the program executes. 
Each fine-grain task of a {\em RambutanAcc} graph is scheduled to run on a {\em worker}, which can be either a group of CPU cores, an accelerator, or a partition of the accelerator.
Tasks can execute as soon as their {\em true} data dependencies are satisfied, thus avoiding many types of over-synchronization present in other programming models (e.g. barrier, wait\_all, etc.).
Our runtime handles the communication among tasks automatically -- in particular, data required by a task can be produced by other tasks running on CPU or accelerator of a local or remote compute nodes.
The runtime transparently moves data to where the dependent task will execute, and
handles communication in the background so that communication overheads can be overlapped with computation of other runnable tasks.
%\sout{Not only can data move, but tasks can also migrate among different memory address spaces via work stealing.
%Specifically idle workers can find chance to steal tasks from other workers, enabling dynamic load balancing}.

{\em RambutanAcc} currently supports NVIDIA's GPUs and the first generation of Intel's Xeon Phi processor (KNC).
In this paper, we present the GPUs support since GPUs code (e.g. CUDA) is substantially different from conventional code running on the CPU.
Our programming model removes the programming burden of launching CUDA kernels and moving data between the host and GPUs.
Each task is specified as a conventional routine parallelized across a thread team.
%In typical CUDA code, running multiple types of tasks requires launching multiple independent kernels, with little flexibility in when tasks of different types may be executed.
%In contrast, 
{\em RambutanAcc} launches a single {\em persistent CUDA kernel} that can service multiple requests from a task scheduler running on the host, thus avoiding the overhead of multiple kernel launches during execution.
The persistent kernel partitions available SMs (streaming multiprocessors) of a GPUs into workers, which have the flexibility to asynchronously service tasks of different types at the same time.
%This method provides much more flexibility to overlap communication with useful computational work.

{\em RambutanAcc} is also equipped with a communication handler to service communication requests across nodes and between the host and GPUs.
This handler utilizes CUDA streams and works asynchronously with the persistent kernel.
%\footnote{For MIC-based workers, we use Intel's COI (coprocessor offload infrastructure) to implement the on-node handler.}
For off-node requests, {\em RambutanAcc} uses GASNet~\cite{Bonachea:2002:gasnet}, a one-sided communication library % , to implement the communication handler.
that provides non-blocking data transfer and low-latency signaling mechanisms. % , allowing inter-process communication to be overlapped with computations efficiently.

We evaluate {\em RambutanAcc} on up to 16 K80 GPUs using 3 HPC benchmarks: Sparse Cholesky Factorization, Communication-Avoiding Cannon's Matrix Multiplication \cite{25Dcannon}, and 3D Stencil.
The results show that the performance improves significantly due to overlapping communication with computation.
{\em RambutanAcc} also hides communication costs that cannot be avoided.
In addition, for Sparse Cholesky Factorization, having multiple workers on a single GPUs allows compute throughput to be increased at low programming cost. 
We hope this result will encourage further research to develop and tune sparse kernels for co-scheduling fine-grained tasks on GPUs.

The contributions of the paper are three-fold.
\begin{itemize}
\item A task scheduler that supports well fine-grained parallelisms on GPUs without algorithmic change
\item A communication handler that hides various communication overheads at modest programming cost
\item A study of communication overlap in the appearance of a communication avoiding technique 
\end{itemize}

The rest of this paper is organized as follows.
Sec.~\ref{sec:motivation} presents the overview of a hybrid system, which is increasingly popular in practice.
In Sec.~\ref{sec:model}, we present the programming model of {\em RambutanAcc}.
Followed by this section is Sec.~\ref{sec:impl}, which discusses the implementation of the associated runtime system.
Sec.~\ref{sec:results} shows experimental results.
Sec.~\ref{sec:related} presents the related work.
We conclude the paper in Sec.~\ref{sec:conclusion}.

